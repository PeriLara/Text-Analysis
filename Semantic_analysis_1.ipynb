{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de sentiments\n",
    "\n",
    "Implémentation d'un prédicteur naïf de la polarité d'une critique de film tirée de IMDB anglais. \n",
    "Le modèle est un modèle de régression logistique traditionnel mais implémenté avec *pytorch*.\n",
    "Le jeu de données est à télécharger depuis le site suivant : http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "random.seed(1) #reproductibility\n",
    "\n",
    "#loads data from disk\n",
    "def load_dataset(dir_path,ref_label, dev_test=False):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a directory path and \n",
    "    returns a list of couples (Counter of Bow_freq,ref_label) one for each text\n",
    "    \"\"\"\n",
    "    dpath    = os.path.abspath(dir_path) \n",
    "    if dev_test:\n",
    "        devset = []\n",
    "        testset = []\n",
    "        for f in os.listdir(dpath)[250:]:\n",
    "            filepath    = os.path.join(dpath,f)\n",
    "            file_stream = open(filepath)\n",
    "            text        = file_stream.read().split()\n",
    "            file_stream.close()\n",
    "            devset.append((Counter(text),ref_label))\n",
    "        for f in os.listdir(dpath)[250:600]:\n",
    "            filepath    = os.path.join(dpath,f)\n",
    "            file_stream = open(filepath)\n",
    "            text        = file_stream.read().split()\n",
    "            file_stream.close()\n",
    "            testset.append((Counter(text),ref_label))\n",
    "        return devset, testset\n",
    "    else:\n",
    "        data_set = [] \n",
    "        for f in os.listdir(dpath):\n",
    "            filepath    = os.path.join(dpath,f)\n",
    "            file_stream = open(filepath)\n",
    "            text        = file_stream.read().split()\n",
    "            file_stream.close()\n",
    "            data_set.append((Counter(text),ref_label))\n",
    "        return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpos = load_dataset(\"/Users/lara/Documents/LI/M2/Analyse_syntaxique/aclImdb/train/pos\",1) \n",
    "trainneg = load_dataset(\"/Users/lara/Documents/LI/M2/Analyse_syntaxique/aclImdb/train/neg\",0)\n",
    "training_dataset  = trainpos\n",
    "training_dataset.extend(trainneg)\n",
    "random.shuffle(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devpos, testpos = load_dataset(\"/Users/lara/Documents/LI/M2/Analyse_syntaxique/aclImdb/test/pos\",1, dev_test=True)\n",
    "devneg, testneg = load_dataset(\"/Users/lara/Documents/LI/M2/Analyse_syntaxique/aclImdb/test/neg\",0, dev_test=True)\n",
    "testing_dataset = testpos\n",
    "dev_dataset = devpos\n",
    "testing_dataset.extend(testneg)\n",
    "dev_dataset.extend(devneg)\n",
    "random.shuffle(testing_dataset)\n",
    "random.shuffle(dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a dict that maps each known word string to a unique integer\n",
    "def make_w2idx(dataset):\n",
    "    wordset = set([])\n",
    "    for X,Y in dataset:\n",
    "        wordset.update([word for word in X])\n",
    "    return dict(zip(wordset,range(len(wordset))))   \n",
    "\n",
    "def vectorize_text(counter,w2idx):\n",
    "    xvec = torch.zeros(len(w2idx))\n",
    "    for word in counter:\n",
    "        if word in w2idx:       #manages unk words (ignored)\n",
    "            xvec[w2idx[word]] = counter[word] \n",
    "    return xvec.squeeze()\n",
    "\n",
    "def vectorize_target(ylabel):\n",
    "     return torch.tensor(float(ylabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle\n",
    "\n",
    "La classe SentimentAnalyzer hérite de la classe torch.nn.Module. C'est une classe de base pour tous les modules du réseau de neurones.\n",
    "La fonction reset_structure prend en arguments le réseau de neurones d'analyse de sentiments, la taille du vocabulaire (ce qui correspond à la taille de l'input) et le nombre de classes (ce qui correspond à la taille de l'output) et applique une transformation linéaire sur les données d'entrée (et donc sur la couche d'entrée) du réseau de neurones en renvoyant la matrice de poids W.\n",
    "La fonction forward applique la propagation avant du réseau de neurones avec la fonction d'activation sigmoid qu'il cherche dans la bibliothèque fonctionnelle des réseaux de neurones de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-77ccd7a47470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class SentimentAnalyzer(nn.Module): \n",
    "    \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(SentimentAnalyzer, self).__init__()\n",
    "            self.reset_structure(1,1)\n",
    "            \n",
    "        def reset_structure(self,vocab_size, num_labels):\n",
    "            \n",
    "            self.W = nn.Linear(vocab_size, num_labels)\n",
    "            \n",
    "        def forward(self, text_vec):\n",
    "            \n",
    "            return F.sigmoid(self.W(text_vec)) #sigmoid is the logistic activation\n",
    "        \n",
    "        def train(self,train_set, dev_set, learning_rate,epochs):\n",
    "            \n",
    "            self.w2idx = make_w2idx(train_set)\n",
    "            self.reset_structure(len(self.w2idx),1)\n",
    "            \n",
    "            #remind that minimizing Binary Cross Entropy <=> minimizing NLL\n",
    "            loss_func  = nn.BCELoss() \n",
    "            optimizer  = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                global_logloss = 0.0\n",
    "                for X, Y in train_set: \n",
    "                    self.zero_grad() # sets gradients of all model parameters to 0\n",
    "                    xvec            = vectorize_text(X,self.w2idx)\n",
    "                    yvec            = vectorize_target(Y)\n",
    "                    prob            = self(xvec).squeeze()\n",
    "                    # appel implicite de la fonction forward --> forward propagation sur vecteur x\n",
    "                    # .squeeze() retourne un tenseur avec toutes les dimensions d'entrée de taille 1 enlevées.\n",
    "                    loss            = loss_func(prob,yvec)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    global_logloss += loss.item()\n",
    "                for X, Y in dev_set: \n",
    "                    # On entraine le réseau sur l'ensemble de developpement\n",
    "                    # mais on ne le modifie pas ( pas de backward propagation )\n",
    "                    self.zero_grad() # sets gradients of all model parameters to 0\n",
    "                    xvec            = vectorize_text(X,self.w2idx)\n",
    "                    yvec            = vectorize_target(Y)\n",
    "                    prob            = self(xvec).squeeze()\n",
    "                    loss            = loss_func(prob,yvec)\n",
    "                    global_logloss += loss.item()\n",
    "\n",
    "                print(\"Epoch %d, mean cross entropy = %f\"%(epoch, global_logloss/len(dev_set)))\n",
    "\n",
    "        def run_test(self,test_set):\n",
    "            # renvoie un score d'accurracy de classification sur un jeu de test.\n",
    "            with torch.no_grad():\n",
    "                acc = 0.0\n",
    "                total = 0.0\n",
    "                for X, Y in test_set:\n",
    "                    total += 1\n",
    "                    xvec = vectorize_text(X,self.w2idx)\n",
    "                    yvec = vectorize_target(Y)\n",
    "                    yhat = self(xvec).squeeze()\n",
    "                    if (yvec ==1 and yhat >= 0.5) or (yvec == 0 and yhat < 0.5):\n",
    "                        acc += 1\n",
    "            return acc * 100 / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inférences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-903ef39884f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mmce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmce\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "i = 0 \n",
    "with open('grid_search.csv', 'w') as csvfile:\n",
    "    grid_search = csv.writer(csvfile, delimiter=\";\")\n",
    "    grid_search.writerow(['GD', 'epochs', 'lr', 'accuracy', 'mean cross entropy at last epoch'])\n",
    "    for epoch in [100, 500, 1000]:\n",
    "        for lr in [0.001, 0.01, 0.1, 1, 10]:\n",
    "            mce = sent.train(training_dataset,dev_dataset, lr,epoch)\n",
    "            acc = sent.run_test(testing_dataset)\n",
    "            grid_search.writerow([i, epoch, lr, acc, mce])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
